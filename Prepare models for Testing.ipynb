{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24762e2b",
   "metadata": {},
   "source": [
    "This is an auxiliary notebook to make the definition of each model easier. With it, models can be defined and saved for later training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ed01f6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T21:59:00.952363Z",
     "start_time": "2022-05-01T21:58:59.599228Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26238dd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T21:59:02.287071Z",
     "start_time": "2022-05-01T21:59:00.986924Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = './datasets/Cancer Detection/'\n",
    "img_dir = data_dir+'labeled images/'\n",
    "dataset = ImageFolder(img_dir, transform=ToTensor())\n",
    "test_image = dataset[0][0].unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc00ad82",
   "metadata": {},
   "source": [
    "We start by defining the model class in the same way that we did in the training notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b19d21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T21:59:02.334593Z",
     "start_time": "2022-05-01T21:59:02.320589Z"
    }
   },
   "outputs": [],
   "source": [
    "class myImageClassification(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        \"\"\"Takes in a batch of training images and returns their loss\"\"\"\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        \"\"\"Takes in a batch of validation images and returns their loss and accuracy\"\"\"\n",
    "        images, labels = batch\n",
    "        out = self(images)\n",
    "        loss = F.cross_entropy(out, labels)\n",
    "        acc = accuracy(out, labels)\n",
    "        tp, tn, fp, fn = confusion(out, labels)\n",
    "        return {'val_loss': loss.detach(), 'val_acc':acc, 'tp':tp, 'tn':tn, 'fp':fp, 'fn':fn} #detach so its gradient won't be calculated, so not taking up vram\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        \"\"\"Takes in the outputs of the validation step and returns the average loss/accuracy for all batches\"\"\"\n",
    "        batch_losses = [x['val_loss'] for x in outputs] #list of validation losses of each batch\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   #average validation loss for all the batches \n",
    "        batch_accs = [x['val_acc'] for x in outputs]    #list of validation accuracy of each batch\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      #average validation accuracy for all the batches\n",
    "        epoch_tp = reduce(lambda a,b: a+b,[x['tp'] for x in outputs])\n",
    "        epoch_tn = reduce(lambda a,b: a+b,[x['tn'] for x in outputs])\n",
    "        epoch_fp = reduce(lambda a,b: a+b,[x['fp'] for x in outputs])\n",
    "        epoch_fn = reduce(lambda a,b: a+b,[x['fn'] for x in outputs])\n",
    "        return {'val_loss':epoch_loss.item(), 'val_acc':epoch_acc.item(), 'tp':epoch_tp, 'tn':epoch_tn, 'fp':epoch_fp, 'fn':epoch_fn}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        \"\"\"Prints the result of an epoch\"\"\"\n",
    "        print(\"\\033[94mEpoch [{}]:\\033[0m\\ntrain_loss: {:.4f}, val_loss: {:.4f} val_acc: {:.4f}\".format(\n",
    "            epoch, result['train_loss'], result['val_loss'], result['val_acc']))\n",
    "        print(print_confusion(result['tp'], result['tn'], result['fp'], result['fn']))\n",
    "        print(report(result['tp'], result['tn'], result['fp'], result['fn']),'\\n')\n",
    "        \n",
    "def accuracy(outputs, labels):\n",
    "    \"\"\"Prints the accuracy\"\"\"\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f8997a",
   "metadata": {},
   "source": [
    "I used auxiliary models to check the output size of layers to make sure that the input of the next ones are set correctly. This can of course also be easily calculated by taking into account the amount of zero padding in convolutions and the size of the pooling kernel, but was especially useful when working without zero padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d2617c9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-01T21:59:08.189324Z",
     "start_time": "2022-05-01T21:59:08.147315Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output: torch.Size([1, 64, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "class aux_model(myImageClassification):\n",
    "    def __init__(self):\n",
    "        \"\"\"Define the model's network architecture\"\"\"\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3,21,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "#             nn.Conv2d(6,11,kernel_size=3,padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(11,17,kernel_size=3,padding=1),\n",
    "#             nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Conv2d(21,43,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "#             nn.Conv2d(23,28,kernel_size=3,padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(28,34,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Conv2d(43,64,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "#             nn.Conv2d(40,46,kernel_size=3,padding=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Conv2d(46,51,kernel_size=3,padding=1),\n",
    "#             nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        \"\"\"Apply the network to the batch of 'x'(images)\"\"\"\n",
    "        return self.network(xb)\n",
    "\n",
    "size_model = aux_model()\n",
    "print('Shape of output:', size_model(test_image).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db459649",
   "metadata": {},
   "source": [
    "The architectures are defined in the below cell by modifying the code inside the nn.Sequential's parentheses and running the cell, after which the architecture is saved in the form of an untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f95b1b03",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-02T12:49:41.886085Z",
     "start_time": "2022-05-02T12:49:41.865784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCamModel(\n",
      "  (network): Sequential(\n",
      "    (0): Conv2d(3, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU()\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (7): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU()\n",
      "    (9): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): ReLU()\n",
      "    (11): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (12): ReLU()\n",
      "    (13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (14): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU()\n",
      "    (16): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU()\n",
      "    (18): Conv2d(14, 14, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (19): ReLU()\n",
      "    (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (21): Flatten(start_dim=1, end_dim=-1)\n",
      "    (22): Linear(in_features=2016, out_features=2, bias=True)\n",
      "    (23): Softmax(dim=1)\n",
      "  )\n",
      ")\n",
      "Shape of output: torch.Size([1, 64, 12, 12])\n"
     ]
    }
   ],
   "source": [
    "class PCamModel(myImageClassification):\n",
    "    def __init__(self):\n",
    "        \"\"\"Define the model's network architecture\"\"\"\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(3,14,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(14,14,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(14,14,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Conv2d(14,14,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(14,14,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(14,14,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Conv2d(14,14,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(14,14,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(14,14,kernel_size=3,padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            nn.Linear(14*12*12, 2),\n",
    "            nn.Softmax(dim=1),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(2048, 256),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(256,2)       \n",
    "        )\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        \"\"\"Apply the network to the batch of 'x'(images)\"\"\"\n",
    "        return self.network(xb)\n",
    "\n",
    "    \n",
    "#INSTANTIATE AND SAVE THE MODEL\n",
    "group = 'flat'\n",
    "folder = './Models/'+group+'/'\n",
    "filename = group+'-128-9'\n",
    "model = PCamModel()\n",
    "print(model)\n",
    "print('Shape of output:', size_model(test_image).shape)\n",
    "with open(folder+filename+'.txt', 'w') as info_file:\n",
    "    info_file.write(str(model))\n",
    "torch.save(model, folder+filename+'.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
